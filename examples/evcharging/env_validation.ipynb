{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts for generating events and running environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import pytz\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from sustaingym.data.load_moer import load_monthly_moer\n",
    "from sustaingym.envs.evcharging import EVChargingEnv, RealTraceGenerator, GMMsTraceGenerator, DiscreteActionWrapper\n",
    "from sustaingym.algorithms.evcharging.baselines import GreedyAlgorithm, MPC, RandomAlgorithm, RLAlgorithm, OfflineOptimal\n",
    "\n",
    "\n",
    "test_ranges = (\n",
    "    ('2019-05-01', '2019-08-31'),\n",
    "    ('2019-09-01', '2019-12-31'),\n",
    "    ('2020-02-01', '2020-05-31'),\n",
    "    ('2021-05-01', '2021-08-31'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run environment Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EVChargingEnv(RealTraceGenerator('caltech', test_ranges[0]))\n",
    "\n",
    "done = False\n",
    "obs, episode_info = env.reset(seed=100, return_info=True)\n",
    "steps = 0\n",
    "while not done:\n",
    "    action = np.ones((54,))\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    steps += 1\n",
    "\n",
    "print(steps)\n",
    "print(info['reward_breakdown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real trace generator + GMMs trace generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check able to generate on all days\n",
    "for site in ['caltech', 'jpl']:\n",
    "    for test_range in test_ranges:\n",
    "        print('testing: ', site, test_range)\n",
    "        rtg = RealTraceGenerator(site, test_range)\n",
    "        for _ in range(123):  # 4 months -> 123 days maximum\n",
    "            _, _, num_plug_events = rtg.get_event_queue()\n",
    "        print(rtg)\n",
    "\n",
    "\n",
    "for site in ['caltech', 'jpl']:\n",
    "    for test_range in test_ranges:\n",
    "        print('testing: ', site, test_range)\n",
    "        gmmg = GMMsTraceGenerator(site, test_range)\n",
    "        for _ in range(123):  # 4 months -> 123 days maximum\n",
    "            _, _, num_plug_events = gmmg.get_event_queue()\n",
    "        print(gmmg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the Environment makes sense\n",
    "- sanity check on rewards for policies\n",
    "- seed setting\n",
    "- printing, __repr__, step, reset, render, close\n",
    "- discrete action wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic\n",
    "Run selective full charge on environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = EVChargingEnv(GMMsTraceGenerator('caltech', test_ranges[0]))\n",
    "print('--- Print environment ---')\n",
    "print(repr(env))\n",
    "done = False\n",
    "\n",
    "obs = env.reset(seed=100)\n",
    "steps = 0\n",
    "while not done:\n",
    "    action = np.where(obs['demands'] > 0, 1, 0)\n",
    "    obs, reward, done, info = env.step(action, return_info=True)\n",
    "    steps += 1\n",
    "\n",
    "try:\n",
    "    env.render()\n",
    "except NotImplementedError:\n",
    "    print(\"Render is not implemented as expected\")\n",
    "env.close()\n",
    "\n",
    "print(\"--- Number of steps taken ---\")\n",
    "print(steps)\n",
    "\n",
    "print(\"--- Print keys for info ---\")\n",
    "print(info.keys())\n",
    "# print('evs: ', info['evs'])\n",
    "print('num_evs: ', info['num_evs'])\n",
    "print('avg_plugin_time: ', info['avg_plugin_time'])\n",
    "print('max_profit: ', info['max_profit'])\n",
    "print('reward_breakdown: ', info['reward_breakdown'])\n",
    "print('moer: ', info['moer'])\n",
    "print('active_evs: ', info['active_evs'])\n",
    "print('pilot_signals: ', info['pilot_signals'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards from policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = ['random', 'full', 'selective_full']\n",
    "env1 = EVChargingEnv(RealTraceGenerator('caltech', test_ranges[0]))\n",
    "env2 = EVChargingEnv(RealTraceGenerator('caltech', test_ranges[0]), project_action_in_env=False)\n",
    "\n",
    "np.random.seed(42)\n",
    "random_action = np.random.randint(0, 5, size=(54,)).astype(float) / 4\n",
    "\n",
    "for policy in policies:\n",
    "    for env in [env1, env2]:\n",
    "        num_episodes = 1\n",
    "        rewards = []\n",
    "        reward_comps = {'profit': 0, 'carbon_cost': 0, 'excess_charge': 0}\n",
    "        for _ in range(num_episodes):\n",
    "            obs, info = env.reset(seed=43, return_info=True)\n",
    "            done = False\n",
    "            tot_reward = 0\n",
    "            timestep = 0\n",
    "            while not done:\n",
    "                timestep += 1\n",
    "                if policy == 'random':\n",
    "                    action = random_action\n",
    "                elif policy == 'full':\n",
    "                    action = np.full((54,), 1)\n",
    "                elif policy == 'none':\n",
    "                    action = np.zeros((54,))\n",
    "                else:\n",
    "                    action = np.where(obs['demands'] > 0, 1, 0)\n",
    "                obs, reward, done, info = env.step(action)\n",
    "\n",
    "                tot_reward += reward\n",
    "                # for reward_comp in info['reward']:\n",
    "                #     reward_comps[reward_comp] += info['reward'][reward_comp]\n",
    "            rewards.append(tot_reward)\n",
    "        print(f'{policy} {env.project_action_in_env}')\n",
    "        \n",
    "        rd = info['reward_breakdown']\n",
    "        print(\"Outside: \", tot_reward, \"Inside: \", rd['profit'] - rd['carbon_cost'] - rd['excess_charge'])\n",
    "        print(\"Best possible: \", info['max_profit'])\n",
    "        print('reward components:', info['reward_breakdown'])  # total reward contribution over num_episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check seed setting is done correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = []\n",
    "for sequential in [False, True]:\n",
    "    gen = RealTraceGenerator('caltech', test_ranges[1], sequential=sequential)\n",
    "    generators.append(gen)\n",
    "\n",
    "gen = GMMsTraceGenerator('caltech', test_ranges[1])\n",
    "generators.append(gen)\n",
    "\n",
    "for gen in generators:\n",
    "    print('--- ', gen)\n",
    "    for seed in [None, 11]:\n",
    "        print(f'seed is {seed}')\n",
    "        for _ in range(3):\n",
    "            gen.set_seed(seed)\n",
    "            days = []\n",
    "            for _ in range(7):\n",
    "                _, _, num_event = gen.get_event_queue()\n",
    "                days.append(gen.day.strftime(\"%m/%d\"))\n",
    "            print(days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Action Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DiscreteActionWrapper(EVChargingEnv(RealTraceGenerator('caltech', test_ranges[0])))\n",
    "\n",
    "action = np.random.randint(0, 5, size=(54,))\n",
    "\n",
    "done = False\n",
    "obs, episode_info = env.reset(seed=100, return_info=True)\n",
    "steps = 0\n",
    "while not done:\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    steps += 1\n",
    "\n",
    "print(steps)\n",
    "print(info['reward_breakdown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Easy Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_discrete = DiscreteActionWrapper(EVChargingEnv(RealTraceGenerator('caltech', test_ranges[2])))\n",
    "env_continuous = EVChargingEnv(RealTraceGenerator('caltech', test_ranges[2]))\n",
    "\n",
    "algorithms = {\n",
    "    'random': RandomAlgorithm,\n",
    "    'mpc': MPC,\n",
    "    'greedy': GreedyAlgorithm,\n",
    "}\n",
    "\n",
    "for k in algorithms:\n",
    "    for action_type, env in zip(['discrete', 'continuous'], [env_discrete, env_continuous]):\n",
    "        try:\n",
    "            algorithm = algorithms[k](env)\n",
    "\n",
    "            reward_breakdown = algorithm.run(seeds=5)\n",
    "            print(f'{k}, {action_type}')\n",
    "            print(reward_breakdown)\n",
    "        except AssertionError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb1 = reward_breakdown.copy()\n",
    "rb2 = reward_breakdown.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame({}), rb1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb1['timestep'] = 25\n",
    "rb2['timestep'] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(rb1['reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Algorithm: Offline Optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_continuous = EVChargingEnv(RealTraceGenerator('caltech', test_ranges[2]))\n",
    "oo = OfflineOptimal(env_continuous)\n",
    "\n",
    "reward_breakdown = oo.run(seeds=[2])\n",
    "print(reward_breakdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#       reward     profit  carbon_cost  excess_charge  max_profit\n",
    "# 0  10.925368  13.099406     2.174038            0.0    14.45262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(54):\n",
    "    plt.plot(oo.traj.value[i])\n",
    "plt.hlines(y=6/32, xmin=0, xmax=288, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(ev.remaining_demand, ev.requested_energy, ev.energy_delivered, ev.departure - ev.arrival) for ev in env_continuous.evs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_continuous.A_PERS_TO_KWH * 32# * env_continuous.ACTION_SCALE_FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_breakdown.to_csv(\"test_for_to_csv.csv\", compression='gzip', index=False)\n",
    "rb = pd.read_csv(\"test_for_to_csv.csv\", compression='gzip')\n",
    "\n",
    "rb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "ga = GreedyAlgorithm(project_action=True)\n",
    "mpc1 = MPC(lookahead=1)\n",
    "mpc2 = MPC(lookahead=2)\n",
    "mpc6 = MPC(lookahead=6)\n",
    "mpc12 = MPC(lookahead=12)\n",
    "mpc36 = MPC(lookahead=36)\n",
    "\n",
    "lbls = ['mpc1']\n",
    "algs = [mpc1]\n",
    "# lbls = ['greedy', 'mpc1', 'mpc2', 'mpc6', 'mpc12', 'mpc36']\n",
    "# algs = [ga, mpc1, mpc2, mpc6, mpc12, mpc36]\n",
    "\n",
    "DEFAULT_DATE_RANGES = (\n",
    "    ('2019-05-01', '2019-08-31'),\n",
    "    ('2019-09-01', '2019-12-31'),\n",
    "    ('2020-02-01', '2020-05-31'),\n",
    "    ('2021-05-01', '2021-08-31'),\n",
    ")\n",
    "\n",
    "# bug fix: commit 18be9933cbf14e19e17332c9a870f480471eea86\n",
    "# 2019-08-14: StationOccupiedError: Station CA-303 is occupied with ev 2_39_139_28_2019-08-15 07:07:28.618042.\n",
    "# 2019-10-25: StationOccupiedError: Station CA-317 is occupied with ev 2_39_91_437_2019-10-26 07:36:37.638121. -> change mask: mask = (self.day.day == max_depart.dt.day) instead of mask = (df['arrival'].dt.day == max_depart.dt.day)\n",
    "# 2020-03-15: StationOccupiedError: Station CA-303 is occupied with ev 2_39_139_28_2020-03-16 07:51:17.415039.\n",
    "\n",
    "DATE_FORMAT = '%Y-%m-%d'\n",
    "def num_days_in_period(xs) -> int:\n",
    "    \"\"\"Returns the number of days in period.\"\"\"\n",
    "    dts = tuple(datetime.strptime(x, DATE_FORMAT) for x in xs)\n",
    "    td = dts[1] - dts[0]\n",
    "    return td.days + 1\n",
    "\n",
    "for alg, lbl in zip(algs, lbls):\n",
    "    for season in DEFAULT_DATE_RANGES:\n",
    "        gen = RealTraceGenerator('caltech', season, sequential=True)\n",
    "        env = EVChargingEnv(gen)\n",
    "        rewards, breakdown = alg.run(num_days_in_period(season), env)\n",
    "\n",
    "        print(f'{lbl} average reward: ', np.mean(rewards))\n",
    "        print(f'{lbl} rewards: ', rewards)\n",
    "        print(f'{lbl} reward breakdown: ', breakdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sustaingym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c910351b0c3a4aade2cf03b555240fe9951314ae7b50b4f56bc279231ceafe8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
