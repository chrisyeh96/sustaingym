{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/sustaingym\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import gymnasium as gym\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms import ppo, AlgorithmConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from sustaingym.envs.evcharging import EVChargingEnv, RealTraceGenerator, GMMsTraceGenerator, DiscreteActionWrapper\n",
    "from sustaingym.envs.evcharging.event_generation import AbstractTraceGenerator\n",
    "from sustaingym.envs.evcharging.utils import \\\n",
    "    DATE_FORMAT, DEFAULT_PERIOD_TO_RANGE, DATE_FORMAT, SiteStr\n",
    "\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "\n",
    "###\n",
    "NUM_SUBPROCESSES = 4\n",
    "TIMESTEPS = 250_000\n",
    "EVAL_FREQ = 10_000\n",
    "SAMPLE_EVAL_PERIODS = {\n",
    "    'Summer 2019':   ('2019-07-01', '2019-07-14'),\n",
    "    'Fall 2019':     ('2019-11-04', '2019-11-17'),\n",
    "    'Spring 2020':   ('2020-04-06', '2020-04-19'),\n",
    "    'Summer 2021':   ('2021-07-05', '2021-07-18'),\n",
    "}\n",
    "\n",
    "def get_env(full: bool, real_trace: bool, dp: str, site: SiteStr, discrete: bool = False, seed: int=None) -> Callable:\n",
    "    \"\"\"Return environment.\n",
    "\n",
    "    Args:\n",
    "        full: if True, use full season; otherwise, use sample 2 weeks\n",
    "        real_trace: choice of generator\n",
    "        dp: 'Summer 2019', 'Fall 2019', 'Spring 2020', 'Summer 2021'\n",
    "        site: 'caltech' or 'jpl'\n",
    "        discrete: whether to wrap environment in discrete action wrapper\n",
    "        seed: seed for GMMs generator\n",
    "    \n",
    "    Returns:\n",
    "        Callable of environment\n",
    "    \"\"\"\n",
    "    date_period = DEFAULT_PERIOD_TO_RANGE[dp] if full else SAMPLE_EVAL_PERIODS[dp]\n",
    "\n",
    "    def _get_env() -> EVChargingEnv:\n",
    "        if real_trace:\n",
    "            gen: AbstractTraceGenerator = RealTraceGenerator(site, date_period)\n",
    "        else:\n",
    "            gen = GMMsTraceGenerator(site, date_period, seed=seed)\n",
    "        \n",
    "        if discrete:\n",
    "            return TimeLimit(DiscreteActionWrapper(EVChargingEnv(gen, vectorize_obs=False)), max_episode_steps=288)\n",
    "        else:\n",
    "            return TimeLimit(EVChargingEnv(gen, vectorize_obs=False), max_episode_steps=288)\n",
    "    return _get_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 02:07:16,117\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24011)\u001b[0m 2023-04-14 02:07:21,962\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=24011)\u001b[0m 2023-04-14 02:07:21,962\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "2023-04-14 02:07:25,820\tINFO trainable.py:172 -- Trainable.setup took 11.636 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.rllib.algorithms import ppo, AlgorithmConfig\n",
    "\n",
    "register_env(\"my_env\", lambda config: get_env(**config)())\n",
    "\n",
    "train_config = (\n",
    "    ppo.PPOConfig()\n",
    "    .environment(\"my_env\", env_config={\n",
    "        \"full\": True,\n",
    "        \"real_trace\": False,\n",
    "        \"dp\": \"Summer 2019\",\n",
    "        \"site\": \"caltech\",\n",
    "        \"discrete\": False,\n",
    "        \"seed\": 123\n",
    "    })\n",
    "    # .framework(\"tf2\")\n",
    "    .training(train_batch_size=10_000)\n",
    ")\n",
    "algo = train_config.build(env=\"my_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env observation space:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:11<00:00,  5.98s/it]\n"
     ]
    }
   ],
   "source": [
    "from sustaingym.algorithms.evcharging.baselines import RLLibAlgorithm\n",
    "\n",
    "env = get_env(full=False, real_trace=True, dp='Summer 2019', site='caltech', discrete=False, seed=True)()\n",
    "rllib_algo = RLLibAlgorithm(env, algo)\n",
    "reward_breakdown = rllib_algo.run(2).to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reward': [7.190068854778987, 8.184220770725473],\n",
       " 'profit': [9.174460603153184, 10.526924921587453],\n",
       " 'carbon_cost': [1.9843570817075287, 2.3426348175286504],\n",
       " 'excess_charge': [3.4666666666666665e-05, 6.933333333333357e-05],\n",
       " 'max_profit': [11.101619999999999, 12.785959999999998]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-04-12 07:34:41</td></tr>\n",
       "<tr><td>Running for: </td><td>00:16:41.48        </td></tr>\n",
       "<tr><td>Memory:      </td><td>8.2/15.3 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/1 GPUs, 0.0/4.69 GiB heap, 0.0/2.34 GiB objects (0.0/1.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>experiment_27971_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 07:34:45,068\tWARNING tune.py:146 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2023-04-12 07:34:46,079\tERROR tune.py:794 -- Trials did not complete: [experiment_27971_00000]\n",
      "2023-04-12 07:34:46,079\tINFO tune.py:798 -- Total run time: 1006.51 seconds (1006.50 seconds for the tuning loop).\n",
      "2023-04-12 07:34:46,080\tWARNING tune.py:804 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.result_grid.ResultGrid object at 0x7f81c7f87040>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "###\n",
    "\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "\n",
    "\n",
    "# def trainable(config: dict):\n",
    "#     checkpoint_dir = tune.get_trial_dir()\n",
    "#     print(checkpoint_dir)\n",
    "#     print(config)\n",
    "\n",
    "# def trainable(config: dict):\n",
    "#     checkpoint_dir = tune.get_trial_dir()\n",
    "#     print(checkpoint_dir)\n",
    "#     print(config)\n",
    "\n",
    "#     train_config = (\n",
    "#         ppo.PPOConfig()\n",
    "#         .environment(\"my_env\", env_config={\n",
    "#             \"full\": True,\n",
    "#             \"real_trace\": False,\n",
    "#             \"dp\": \"Summer 2019\",\n",
    "#             \"site\": \"caltech\",\n",
    "#             \"discrete\": False,\n",
    "#             \"seed\": 123\n",
    "#         })\n",
    "#         .framework(\"tf2\")\n",
    "#     )\n",
    "#     algo = train_config.build(env=\"my_env\")\n",
    "\n",
    "#     for i in range(2):\n",
    "#         train_results = algo.train()\n",
    "\n",
    "#         algo.\n",
    "\n",
    "\n",
    "def experiment(config):\n",
    "\n",
    "    algo = train_config.build(env=\"my_env\")\n",
    "    print(\"algo built\")\n",
    "    for i in range(1):\n",
    "        print(\"begin training algo\")\n",
    "        train_results = algo.train()\n",
    "        print(\"done training algo\")\n",
    "        print(train_results['agent_timesteps_total'])\n",
    "        print(train_results['custom_metrics'])\n",
    "        print(train_results['episode_reward_max'])\n",
    "        print(train_results['episode_reward_mean'])\n",
    "        print(train_results['episode_reward_min'])\n",
    "\n",
    "        # print(pretty_print(train_results))\n",
    "        algo.save(checkpoint_dir)\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "        tune.report({**train_results, \"a\": 2})\n",
    "    algo.stop()\n",
    "\n",
    "    # Manual Eval\n",
    "    eval_config = (\n",
    "        ppo.PPOConfig()\n",
    "        .environment(\"my_env\", env_config={\n",
    "            \"full\": False,\n",
    "            \"real_trace\": True,\n",
    "            \"dp\": \"Summer 2019\",\n",
    "            \"site\": \"caltech\",\n",
    "            \"discrete\": False,\n",
    "            \"seed\": 123\n",
    "        })\n",
    "    )\n",
    "    eval_algo = eval_config.build(env=\"my_env\")\n",
    "    eval_algo.load_checkpoint(checkpoint_dir)\n",
    "    env = eval_algo.workers.local_worker().env\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    eval_results = {\"eval_reward\": 0, \"eval_eps_length\": 0}\n",
    "    while not done:\n",
    "        action = eval_algo.compute_single_action(obs)\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        eval_results[\"eval_reward\"] += reward\n",
    "        eval_results[\"eval_eps_length\"] += 1\n",
    "    eval_algo.stop()\n",
    "    results = {**train_results, **eval_results}\n",
    "    print(results)\n",
    "    tune.report({**results, \"a\": 3})\n",
    "\n",
    "import os\n",
    "\n",
    "# ray.init(num_cpus=3)\n",
    "# register_env(\"my_env\", lambda config: get_env(**config)())\n",
    "\n",
    "resources = ppo.PPO.default_resource_request(ppo.PPOConfig())\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    # experiment\n",
    "    tune.with_resources(experiment, resources=resources),\n",
    "    param_space={}\n",
    ")\n",
    "final_results = tuner.fit()\n",
    "\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
