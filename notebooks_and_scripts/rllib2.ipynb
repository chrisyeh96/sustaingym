{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/sustaingym\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Union\n",
    "\n",
    "import gymnasium as gym\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms import ppo, AlgorithmConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from sustaingym.envs.evcharging import EVChargingEnv, RealTraceGenerator, GMMsTraceGenerator, DiscreteActionWrapper\n",
    "from sustaingym.envs.evcharging.event_generation import AbstractTraceGenerator\n",
    "from sustaingym.envs.evcharging.utils import \\\n",
    "    DATE_FORMAT, DEFAULT_PERIOD_TO_RANGE, DATE_FORMAT, SiteStr\n",
    "\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "\n",
    "\n",
    "###\n",
    "NUM_SUBPROCESSES = 4\n",
    "TIMESTEPS = 250_000\n",
    "EVAL_FREQ = 10_000\n",
    "SAMPLE_EVAL_PERIODS = {\n",
    "    'Summer 2019':   ('2019-07-01', '2019-07-14'),\n",
    "    'Fall 2019':     ('2019-11-04', '2019-11-17'),\n",
    "    'Spring 2020':   ('2020-04-06', '2020-04-19'),\n",
    "    'Summer 2021':   ('2021-07-05', '2021-07-18'),\n",
    "}\n",
    "\n",
    "def get_env(full: bool, real_trace: bool, dp: str, site: SiteStr, discrete: bool = False, seed: int=None) -> Callable:\n",
    "    \"\"\"Return environment.\n",
    "\n",
    "    Args:\n",
    "        full: if True, use full season; otherwise, use sample 2 weeks\n",
    "        real_trace: choice of generator\n",
    "        dp: 'Summer 2019', 'Fall 2019', 'Spring 2020', 'Summer 2021'\n",
    "        site: 'caltech' or 'jpl'\n",
    "        discrete: whether to wrap environment in discrete action wrapper\n",
    "        seed: seed for GMMs generator\n",
    "    \n",
    "    Returns:\n",
    "        Callable of environment\n",
    "    \"\"\"\n",
    "    date_period = DEFAULT_PERIOD_TO_RANGE[dp] if full else SAMPLE_EVAL_PERIODS[dp]\n",
    "\n",
    "    def _get_env() -> EVChargingEnv:\n",
    "        if real_trace:\n",
    "            gen: AbstractTraceGenerator = RealTraceGenerator(site, date_period)\n",
    "        else:\n",
    "            gen = GMMsTraceGenerator(site, date_period, seed=seed)\n",
    "        \n",
    "        if discrete:\n",
    "            return TimeLimit(DiscreteActionWrapper(EVChargingEnv(gen)), max_episode_steps=288)\n",
    "        else:\n",
    "            return TimeLimit(EVChargingEnv(gen), max_episode_steps=288)\n",
    "    return _get_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 04:43:52,816\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=38135)\u001b[0m 2023-04-06 04:43:59,003\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=38135)\u001b[0m 2023-04-06 04:43:59,003\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "2023-04-06 04:44:01,653\tINFO trainable.py:172 -- Trainable.setup took 10.792 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "from ray.air import session\n",
    "from ray.rllib.algorithms import ppo, AlgorithmConfig\n",
    "\n",
    "# ray.init(num_cpus=3)\n",
    "register_env(\"my_env\", lambda config: get_env(**config)())\n",
    "\n",
    "train_config = (\n",
    "    ppo.PPOConfig()\n",
    "    .environment(\"my_env\", env_config={\n",
    "        \"full\": True,\n",
    "        \"real_trace\": False,\n",
    "        \"dp\": \"Summer 2019\",\n",
    "        \"site\": \"caltech\",\n",
    "        \"discrete\": False,\n",
    "        \"seed\": 123\n",
    "    })\n",
    "    .framework(\"tf2\")\n",
    ")\n",
    "algo = train_config.build(env=\"my_env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env observation space:  (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:46<00:00,  7.63s/it]\n"
     ]
    }
   ],
   "source": [
    "from sustaingym.algorithms.evcharging.baselines import RLLibAlgorithm\n",
    "\n",
    "env = get_env(full=False, real_trace=True, dp='Summer 2019', site='caltech', discrete=False, seed=True)()\n",
    "rllib_algo = RLLibAlgorithm(env, algo)\n",
    "reward_breakdown = rllib_algo.run(14).to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 22:28:37,713\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "2023-04-05 22:28:38,690\tINFO algorithm_config.py:2899 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2023-04-05 22:28:38,691\tINFO algorithm_config.py:2899 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-04-05 22:30:45</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:06.52        </td></tr>\n",
       "<tr><td>Memory:      </td><td>2.7/15.3 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/3 CPUs, 0/1 GPUs, 0.0/7.56 GiB heap, 0.0/3.78 GiB objects (0.0/1.0 accelerator_type:T4)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                    </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>experiment_3628c_00000</td><td style=\"text-align: right;\">           1</td><td>/home/ubuntu/ray_results/experiment_2023-04-05_22-28-38/experiment_3628c_00000_0_2023-04-05_22-28-38/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>experiment_3628c_00000</td><td>ERROR   </td><td>172.31.19.228:31245</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         113.219</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m 2023-04-05 22:28:44,113\tINFO algorithm_config.py:2888 -- Executing eagerly (framework='tf2'), with eager_tracing=tf2. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m 2023-04-05 22:28:44,280\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m /home/ubuntu/ray_results/experiment_2023-04-05_22-28-38/experiment_3628c_00000_0_2023-04-05_22-28-38/\n",
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=31309)\u001b[0m 2023-04-05 22:28:49,385\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=31309)\u001b[0m 2023-04-05 22:28:49,385\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m algo built\n",
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m begin training algo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m 2023-04-05 22:29:52,511\tWARNING deprecation.py:50 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>_metric                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>hostname        </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip      </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>experiment_3628c_00000</td><td>{&#x27;custom_metrics&#x27;: {}, &#x27;episode_media&#x27;: {}, &#x27;info&#x27;: {&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;cur_kl_coeff&#x27;: 0.20000000298023224, &#x27;cur_lr&#x27;: 4.999999873689376e-05, &#x27;total_loss&#x27;: 0.17219092, &#x27;policy_loss&#x27;: -0.07289077, &#x27;vf_loss&#x27;: 0.24229585, &#x27;vf_explained_var&#x27;: 0.49599043, &#x27;kl&#x27;: 0.013929113, &#x27;entropy&#x27;: 76.49437, &#x27;entropy_coeff&#x27;: 0.0}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 125.0, &#x27;num_grad_updates_lifetime&#x27;: 480.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 479.5}}, &#x27;num_env_steps_sampled&#x27;: 4000, &#x27;num_env_steps_trained&#x27;: 4000, &#x27;num_agent_steps_sampled&#x27;: 4000, &#x27;num_agent_steps_trained&#x27;: 4000}, &#x27;sampler_results&#x27;: {&#x27;episode_reward_max&#x27;: 3.93224650215404, &#x27;episode_reward_min&#x27;: 0.7552686244731401, &#x27;episode_reward_mean&#x27;: 2.4892613019999303, &#x27;episode_len_mean&#x27;: 288.0, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 12, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [3.0030362793230005, 3.93224650215404, 3.4619184923529542, 2.742651249146179, 2.4252262847728034, 0.7807668284229972, 1.9450977753402676, 2.5155639745967204, 0.7552686244731401, 2.5493733043664486, 2.1435935163188797, 3.6163927927317348], &#x27;episode_lengths&#x27;: [288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.5527151518616302, &#x27;mean_inference_ms&#x27;: 7.320524215221642, &#x27;mean_action_processing_ms&#x27;: 0.2390582939197516, &#x27;mean_env_wait_ms&#x27;: 22.970346556134007, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.010246038436889648, &#x27;StateBufferConnector_ms&#x27;: 0.007273753484090169, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.15441377957661948}}, &#x27;episode_reward_max&#x27;: 3.93224650215404, &#x27;episode_reward_min&#x27;: 0.7552686244731401, &#x27;episode_reward_mean&#x27;: 2.4892613019999303, &#x27;episode_len_mean&#x27;: 288.0, &#x27;episodes_this_iter&#x27;: 12, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [3.0030362793230005, 3.93224650215404, 3.4619184923529542, 2.742651249146179, 2.4252262847728034, 0.7807668284229972, 1.9450977753402676, 2.5155639745967204, 0.7552686244731401, 2.5493733043664486, 2.1435935163188797, 3.6163927927317348], &#x27;episode_lengths&#x27;: [288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.5527151518616302, &#x27;mean_inference_ms&#x27;: 7.320524215221642, &#x27;mean_action_processing_ms&#x27;: 0.2390582939197516, &#x27;mean_env_wait_ms&#x27;: 22.970346556134007, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: 0.010246038436889648, &#x27;StateBufferConnector_ms&#x27;: 0.007273753484090169, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.15441377957661948}, &#x27;num_healthy_workers&#x27;: 2, &#x27;num_in_flight_async_reqs&#x27;: 0, &#x27;num_remote_worker_restarts&#x27;: 0, &#x27;num_agent_steps_sampled&#x27;: 4000, &#x27;num_agent_steps_trained&#x27;: 4000, &#x27;num_env_steps_sampled&#x27;: 4000, &#x27;num_env_steps_trained&#x27;: 4000, &#x27;num_env_steps_sampled_this_iter&#x27;: 4000, &#x27;num_env_steps_trained_this_iter&#x27;: 4000, &#x27;timesteps_total&#x27;: 4000, &#x27;num_steps_trained_this_iter&#x27;: 4000, &#x27;agent_timesteps_total&#x27;: 4000, &#x27;timers&#x27;: {&#x27;training_iteration_time_ms&#x27;: 107066.006, &#x27;learn_time_ms&#x27;: 44783.599, &#x27;learn_throughput&#x27;: 89.318, &#x27;synch_weights_time_ms&#x27;: 4.836}, &#x27;counters&#x27;: {&#x27;num_env_steps_sampled&#x27;: 4000, &#x27;num_env_steps_trained&#x27;: 4000, &#x27;num_agent_steps_sampled&#x27;: 4000, &#x27;num_agent_steps_trained&#x27;: 4000}, &#x27;done&#x27;: False, &#x27;episodes_total&#x27;: 12, &#x27;training_iteration&#x27;: 1, &#x27;trial_id&#x27;: &#x27;default&#x27;, &#x27;experiment_id&#x27;: &#x27;256cbc30557243c0a9dcb4c1fc699e5d&#x27;, &#x27;date&#x27;: &#x27;2023-04-05_22-30-37&#x27;, &#x27;timestamp&#x27;: 1680733837, &#x27;time_this_iter_s&#x27;: 107.0723135471344, &#x27;time_total_s&#x27;: 107.0723135471344, &#x27;pid&#x27;: 31245, &#x27;hostname&#x27;: &#x27;ip-172-31-19-228&#x27;, &#x27;node_ip&#x27;: &#x27;172.31.19.228&#x27;, &#x27;config&#x27;: {&#x27;extra_python_environs_for_driver&#x27;: {}, &#x27;extra_python_environs_for_worker&#x27;: {}, &#x27;num_gpus&#x27;: 0, &#x27;num_cpus_per_worker&#x27;: 1, &#x27;num_gpus_per_worker&#x27;: 0, &#x27;_fake_gpus&#x27;: False, &#x27;num_trainer_workers&#x27;: 0, &#x27;num_gpus_per_trainer_worker&#x27;: 0, &#x27;num_cpus_per_trainer_worker&#x27;: 1, &#x27;custom_resources_per_worker&#x27;: {}, &#x27;placement_strategy&#x27;: &#x27;PACK&#x27;, &#x27;eager_tracing&#x27;: False, &#x27;eager_max_retraces&#x27;: 20, &#x27;tf_session_args&#x27;: {&#x27;intra_op_parallelism_threads&#x27;: 2, &#x27;inter_op_parallelism_threads&#x27;: 2, &#x27;gpu_options&#x27;: {&#x27;allow_growth&#x27;: True}, &#x27;log_device_placement&#x27;: False, &#x27;device_count&#x27;: {&#x27;CPU&#x27;: 1}, &#x27;allow_soft_placement&#x27;: True}, &#x27;local_tf_session_args&#x27;: {&#x27;intra_op_parallelism_threads&#x27;: 8, &#x27;inter_op_parallelism_threads&#x27;: 8}, &#x27;env&#x27;: &#x27;my_env&#x27;, &#x27;env_config&#x27;: {&#x27;full&#x27;: True, &#x27;real_trace&#x27;: False, &#x27;dp&#x27;: &#x27;Summer 2019&#x27;, &#x27;site&#x27;: &#x27;caltech&#x27;, &#x27;discrete&#x27;: False, &#x27;seed&#x27;: 123}, &#x27;observation_space&#x27;: None, &#x27;action_space&#x27;: None, &#x27;env_task_fn&#x27;: None, &#x27;render_env&#x27;: False, &#x27;clip_rewards&#x27;: None, &#x27;normalize_actions&#x27;: True, &#x27;clip_actions&#x27;: False, &#x27;disable_env_checking&#x27;: False, &#x27;is_atari&#x27;: False, &#x27;auto_wrap_old_gym_envs&#x27;: True, &#x27;num_envs_per_worker&#x27;: 1, &#x27;sample_collector&#x27;: &lt;class &#x27;ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector&#x27;&gt;, &#x27;sample_async&#x27;: False, &#x27;enable_connectors&#x27;: True, &#x27;rollout_fragment_length&#x27;: &#x27;auto&#x27;, &#x27;batch_mode&#x27;: &#x27;truncate_episodes&#x27;, &#x27;remote_worker_envs&#x27;: False, &#x27;remote_env_batch_wait_ms&#x27;: 0, &#x27;validate_workers_after_construction&#x27;: True, &#x27;ignore_worker_failures&#x27;: False, &#x27;recreate_failed_workers&#x27;: False, &#x27;restart_failed_sub_environments&#x27;: False, &#x27;num_consecutive_worker_failures_tolerance&#x27;: 100, &#x27;preprocessor_pref&#x27;: &#x27;deepmind&#x27;, &#x27;observation_filter&#x27;: &#x27;NoFilter&#x27;, &#x27;synchronize_filters&#x27;: True, &#x27;compress_observations&#x27;: False, &#x27;enable_tf1_exec_eagerly&#x27;: False, &#x27;sampler_perf_stats_ema_coef&#x27;: None, &#x27;worker_health_probe_timeout_s&#x27;: 60, &#x27;worker_restore_timeout_s&#x27;: 1800, &#x27;gamma&#x27;: 0.99, &#x27;lr&#x27;: 5e-05, &#x27;train_batch_size&#x27;: 4000, &#x27;model&#x27;: {&#x27;_disable_preprocessor_api&#x27;: False, &#x27;_disable_action_flattening&#x27;: False, &#x27;fcnet_hiddens&#x27;: [256, 256], &#x27;fcnet_activation&#x27;: &#x27;tanh&#x27;, &#x27;conv_filters&#x27;: None, &#x27;conv_activation&#x27;: &#x27;relu&#x27;, &#x27;post_fcnet_hiddens&#x27;: [], &#x27;post_fcnet_activation&#x27;: &#x27;relu&#x27;, &#x27;free_log_std&#x27;: False, &#x27;no_final_linear&#x27;: False, &#x27;vf_share_layers&#x27;: False, &#x27;use_lstm&#x27;: False, &#x27;max_seq_len&#x27;: 20, &#x27;lstm_cell_size&#x27;: 256, &#x27;lstm_use_prev_action&#x27;: False, &#x27;lstm_use_prev_reward&#x27;: False, &#x27;_time_major&#x27;: False, &#x27;use_attention&#x27;: False, &#x27;attention_num_transformer_units&#x27;: 1, &#x27;attention_dim&#x27;: 64, &#x27;attention_num_heads&#x27;: 1, &#x27;attention_head_dim&#x27;: 32, &#x27;attention_memory_inference&#x27;: 50, &#x27;attention_memory_training&#x27;: 50, &#x27;attention_position_wise_mlp_dim&#x27;: 32, &#x27;attention_init_gru_gate_bias&#x27;: 2.0, &#x27;attention_use_n_prev_actions&#x27;: 0, &#x27;attention_use_n_prev_rewards&#x27;: 0, &#x27;framestack&#x27;: True, &#x27;dim&#x27;: 84, &#x27;grayscale&#x27;: False, &#x27;zero_mean&#x27;: True, &#x27;custom_model&#x27;: None, &#x27;custom_model_config&#x27;: {}, &#x27;custom_action_dist&#x27;: None, &#x27;custom_preprocessor&#x27;: None, &#x27;lstm_use_prev_action_reward&#x27;: -1, &#x27;_use_default_native_models&#x27;: -1}, &#x27;optimizer&#x27;: {}, &#x27;max_requests_in_flight_per_sampler_worker&#x27;: 2, &#x27;rl_trainer_class&#x27;: None, &#x27;_enable_rl_trainer_api&#x27;: False, &#x27;_rl_trainer_hps&#x27;: RLTrainerHPs(), &#x27;explore&#x27;: True, &#x27;exploration_config&#x27;: {&#x27;type&#x27;: &#x27;StochasticSampling&#x27;}, &#x27;policies&#x27;: {&#x27;default_policy&#x27;: &lt;ray.rllib.policy.policy.PolicySpec object at 0x7fb873d96310&gt;}, &#x27;policy_states_are_swappable&#x27;: False, &#x27;input_config&#x27;: {}, &#x27;actions_in_input_normalized&#x27;: False, &#x27;postprocess_inputs&#x27;: False, &#x27;shuffle_buffer_size&#x27;: 0, &#x27;output&#x27;: None, &#x27;output_config&#x27;: {}, &#x27;output_compress_columns&#x27;: [&#x27;obs&#x27;, &#x27;new_obs&#x27;], &#x27;output_max_file_size&#x27;: 67108864, &#x27;offline_sampling&#x27;: False, &#x27;evaluation_interval&#x27;: None, &#x27;evaluation_duration&#x27;: 10, &#x27;evaluation_duration_unit&#x27;: &#x27;episodes&#x27;, &#x27;evaluation_sample_timeout_s&#x27;: 180.0, &#x27;evaluation_parallel_to_training&#x27;: False, &#x27;evaluation_config&#x27;: None, &#x27;off_policy_estimation_methods&#x27;: {}, &#x27;ope_split_batch_by_episode&#x27;: True, &#x27;evaluation_num_workers&#x27;: 0, &#x27;always_attach_evaluation_results&#x27;: False, &#x27;enable_async_evaluation&#x27;: False, &#x27;in_evaluation&#x27;: False, &#x27;sync_filters_on_rollout_workers_timeout_s&#x27;: 60.0, &#x27;keep_per_episode_custom_metrics&#x27;: False, &#x27;metrics_episode_collection_timeout_s&#x27;: 60.0, &#x27;metrics_num_episodes_for_smoothing&#x27;: 100, &#x27;min_time_s_per_iteration&#x27;: None, &#x27;min_train_timesteps_per_iteration&#x27;: 0, &#x27;min_sample_timesteps_per_iteration&#x27;: 0, &#x27;export_native_model_files&#x27;: False, &#x27;checkpoint_trainable_policies_only&#x27;: False, &#x27;logger_creator&#x27;: None, &#x27;logger_config&#x27;: None, &#x27;log_level&#x27;: &#x27;WARN&#x27;, &#x27;log_sys_usage&#x27;: True, &#x27;fake_sampler&#x27;: False, &#x27;seed&#x27;: None, &#x27;worker_cls&#x27;: None, &#x27;rl_module_class&#x27;: None, &#x27;_enable_rl_module_api&#x27;: False, &#x27;_tf_policy_handles_more_than_one_loss&#x27;: False, &#x27;_disable_preprocessor_api&#x27;: False, &#x27;_disable_action_flattening&#x27;: False, &#x27;_disable_execution_plan_api&#x27;: True, &#x27;simple_optimizer&#x27;: True, &#x27;replay_sequence_length&#x27;: None, &#x27;horizon&#x27;: -1, &#x27;soft_horizon&#x27;: -1, &#x27;no_done_at_end&#x27;: -1, &#x27;lr_schedule&#x27;: None, &#x27;use_critic&#x27;: True, &#x27;use_gae&#x27;: True, &#x27;kl_coeff&#x27;: 0.2, &#x27;sgd_minibatch_size&#x27;: 128, &#x27;num_sgd_iter&#x27;: 30, &#x27;shuffle_sequences&#x27;: True, &#x27;vf_loss_coeff&#x27;: 1.0, &#x27;entropy_coeff&#x27;: 0.0, &#x27;entropy_coeff_schedule&#x27;: None, &#x27;clip_param&#x27;: 0.3, &#x27;vf_clip_param&#x27;: 10.0, &#x27;grad_clip&#x27;: None, &#x27;kl_target&#x27;: 0.01, &#x27;vf_share_layers&#x27;: -1, &#x27;lambda&#x27;: 1.0, &#x27;input&#x27;: &#x27;sampler&#x27;, &#x27;multiagent&#x27;: {&#x27;policies&#x27;: {&#x27;default_policy&#x27;: (None, None, None, None)}, &#x27;policy_mapping_fn&#x27;: &lt;function AlgorithmConfig.__init__.&lt;locals&gt;.&lt;lambda&gt; at 0x7fb873efc160&gt;, &#x27;policies_to_train&#x27;: None, &#x27;policy_map_capacity&#x27;: 100, &#x27;policy_map_cache&#x27;: -1, &#x27;count_steps_by&#x27;: &#x27;env_steps&#x27;, &#x27;observation_fn&#x27;: None}, &#x27;callbacks&#x27;: &lt;class &#x27;ray.rllib.algorithms.callbacks.DefaultCallbacks&#x27;&gt;, &#x27;create_env_on_driver&#x27;: False, &#x27;custom_eval_function&#x27;: None, &#x27;framework&#x27;: &#x27;tf2&#x27;, &#x27;num_cpus_for_driver&#x27;: 1, &#x27;num_workers&#x27;: 2}, &#x27;time_since_restore&#x27;: 107.0723135471344, &#x27;timesteps_since_restore&#x27;: 0, &#x27;iterations_since_restore&#x27;: 1, &#x27;warmup_time&#x27;: 5.951690912246704, &#x27;perf&#x27;: {&#x27;cpu_util_percent&#x27;: 38.81527777777777, &#x27;ram_util_percent&#x27;: 22.171527777777776, &#x27;gpu_util_percent0&#x27;: 0.0, &#x27;vram_util_percent0&#x27;: 0.00013020833333333333}, &#x27;a&#x27;: 2}</td><td>2023-04-05_22-30-37</td><td>False </td><td>                </td><td>f1c1cd3291d043e886975cd22677bdeb</td><td>ip-172-31-19-228</td><td style=\"text-align: right;\">                         1</td><td>172.31.19.228</td><td style=\"text-align: right;\">31245</td><td style=\"text-align: right;\">             113.219</td><td style=\"text-align: right;\">           113.219</td><td style=\"text-align: right;\">       113.219</td><td style=\"text-align: right;\"> 1680733837</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>3628c_00000</td><td style=\"text-align: right;\">   0.00292063</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-05 22:30:37,359\tWARNING tensorboardx.py:224 -- You are trying to log an invalid value (ray/tune/_metric/config/output_compress_columns=['obs', 'new_obs']) via TBXLoggerCallback!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m done training algo\n",
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m 4000\n",
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m {}\n",
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m 3.93224650215404\n",
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m 2.4892613019999303\n",
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m 0.7552686244731401\n",
      "\u001b[2m\u001b[36m(experiment pid=31245)\u001b[0m Checkpoint saved in directory /home/ubuntu/ray_results/experiment_2023-04-05_22-28-38/experiment_3628c_00000_0_2023-04-05_22-28-38/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=37156)\u001b[0m 2023-04-05 22:30:42,949\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=37156)\u001b[0m 2023-04-05 22:30:42,949\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "2023-04-05 22:30:45,247\tERROR tune.py:794 -- Trials did not complete: [experiment_3628c_00000]\n",
      "2023-04-05 22:30:45,249\tINFO tune.py:798 -- Total run time: 126.55 seconds (126.48 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ray.tune.result_grid.ResultGrid object at 0x7fb7ff34a760>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "###\n",
    "\n",
    "from ray import tune\n",
    "from ray.air import session\n",
    "\n",
    "\n",
    "def trainable(config: dict):\n",
    "    checkpoint_dir = tune.get_trial_dir()\n",
    "    print(checkpoint_dir)\n",
    "    print(config)\n",
    "\n",
    "def trainable(config: dict):\n",
    "    checkpoint_dir = tune.get_trial_dir()\n",
    "    print(checkpoint_dir)\n",
    "    print(config)\n",
    "\n",
    "    train_config = (\n",
    "        ppo.PPOConfig()\n",
    "        .environment(\"my_env\", env_config={\n",
    "            \"full\": True,\n",
    "            \"real_trace\": False,\n",
    "            \"dp\": \"Summer 2019\",\n",
    "            \"site\": \"caltech\",\n",
    "            \"discrete\": False,\n",
    "            \"seed\": 123\n",
    "        })\n",
    "        .framework(\"tf2\")\n",
    "    )\n",
    "    algo = train_config.build(env=\"my_env\")\n",
    "\n",
    "    for i in range(2):\n",
    "        train_results = algo.train()\n",
    "\n",
    "        algo.\n",
    "\n",
    "def experiment(config):\n",
    "\n",
    "    algo = train_config.build(env=\"my_env\")\n",
    "    print(\"algo built\")\n",
    "    for i in range(1):\n",
    "        print(\"begin training algo\")\n",
    "        train_results = algo.train()\n",
    "        print(\"done training algo\")\n",
    "        print(train_results['agent_timesteps_total'])\n",
    "        print(train_results['custom_metrics'])\n",
    "        print(train_results['episode_reward_max'])\n",
    "        print(train_results['episode_reward_mean'])\n",
    "        print(train_results['episode_reward_min'])\n",
    "\n",
    "        # print(pretty_print(train_results))\n",
    "        algo.save(checkpoint_dir)\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")\n",
    "        tune.report({**train_results, \"a\": 2})\n",
    "    algo.stop()\n",
    "\n",
    "    # Manual Eval\n",
    "    eval_config = (\n",
    "        ppo.PPOConfig()\n",
    "        .environment(\"my_env\", env_config={\n",
    "            \"full\": False,\n",
    "            \"real_trace\": True,\n",
    "            \"dp\": \"Summer 2019\",\n",
    "            \"site\": \"caltech\",\n",
    "            \"discrete\": False,\n",
    "            \"seed\": 123\n",
    "        })\n",
    "    )\n",
    "    eval_algo = eval_config.build(env=\"my_env\")\n",
    "    eval_algo.load_checkpoint(checkpoint_dir)\n",
    "    env = eval_algo.workers.local_worker().env\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    eval_results = {\"eval_reward\": 0, \"eval_eps_length\": 0}\n",
    "    while not done:\n",
    "        action = eval_algo.compute_single_action(obs)\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        eval_results[\"eval_reward\"] += reward\n",
    "        eval_results[\"eval_eps_length\"] += 1\n",
    "    eval_algo.stop()\n",
    "    results = {**train_results, **eval_results}\n",
    "    print(results)\n",
    "    tune.report({**results, \"a\": 3})\n",
    "\n",
    "import os\n",
    "\n",
    "ray.init(num_cpus=3)\n",
    "register_env(\"my_env\", lambda config: get_env(**config)())\n",
    "\n",
    "resources = ppo.PPO.default_resource_request(ppo.PPOConfig())\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    # experiment\n",
    "    tune.with_resources(experiment, resources=resources),\n",
    "    param_space={}\n",
    ")\n",
    "final_results = tuner.fit()\n",
    "\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/rllib2/lib/python3.9/site-packages/pandas/core/indexes/base.py:2898\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2898\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   2899\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:101\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:1675\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:1683\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'a'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m {result\u001b[39m.\u001b[39mlog_dir: result\u001b[39m.\u001b[39mmetrics_dataframe[\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m final_results}\n",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m {result\u001b[39m.\u001b[39mlog_dir: result\u001b[39m.\u001b[39;49mmetrics_dataframe[\u001b[39m'\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m final_results}\n",
      "File \u001b[0;32m~/.conda/envs/rllib2/lib/python3.9/site-packages/pandas/core/frame.py:2906\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2904\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2905\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 2906\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   2907\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   2908\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.conda/envs/rllib2/lib/python3.9/site-packages/pandas/core/indexes/base.py:2900\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   2899\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 2900\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   2902\u001b[0m \u001b[39mif\u001b[39;00m tolerance \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2903\u001b[0m     tolerance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_tolerance(tolerance, np\u001b[39m.\u001b[39masarray(key))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'a'"
     ]
    }
   ],
   "source": [
    "{result.log_dir: result.metrics_dataframe['a'] for result in final_results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0652830457648164,\n",
       " 0.644091722092978,\n",
       " 1.0565137843837575,\n",
       " 3.6352404391060613,\n",
       " 2.53774750717975,\n",
       " 3.2504868899700825,\n",
       " 1.9874198440814987,\n",
       " 0.7707253657578359,\n",
       " 2.797307384827306,\n",
       " 2.5483773451139538,\n",
       " 3.9048278693570166,\n",
       " 0.5053711386258443]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "final_results.get_best_result().metrics['hist_stats']['episode_reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'cur_kl_coeff': 0.20000000298023224, 'cur_lr': 4.999999873689376e-05, 'total_loss': 0.118393734, 'policy_loss': -0.073018864, 'vf_loss': 0.18794881, 'vf_explained_var': 0.49560168, 'kl': 0.017318841, 'entropy': 76.6001, 'entropy_coeff': 0.0}, 'custom_metrics': {}, 'num_agent_steps_trained': 125.0, 'num_grad_updates_lifetime': 480.5, 'diff_num_grad_updates_vs_sampler_policy': 479.5}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'sampler_results': {'episode_reward_max': 3.9048278693570166, 'episode_reward_min': 0.5053711386258443, 'episode_reward_mean': 2.2252826946884086, 'episode_len_mean': 288.0, 'episode_media': {}, 'episodes_this_iter': 12, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [3.0652830457648164, 0.644091722092978, 1.0565137843837575, 3.6352404391060613, 2.53774750717975, 3.2504868899700825, 1.9874198440814987, 0.7707253657578359, 2.797307384827306, 2.5483773451139538, 3.9048278693570166, 0.5053711386258443], 'episode_lengths': [288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.5278970407641334, 'mean_inference_ms': 7.27252415690882, 'mean_action_processing_ms': 0.23396732448518787, 'mean_env_wait_ms': 23.34069282278188, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00899831453959147, 'StateBufferConnector_ms': 0.00612338383992513, 'ViewRequirementAgentConnector_ms': 0.1399695873260498}}, 'episode_reward_max': 3.9048278693570166, 'episode_reward_min': 0.5053711386258443, 'episode_reward_mean': 2.2252826946884086, 'episode_len_mean': 288.0, 'episodes_this_iter': 12, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [3.0652830457648164, 0.644091722092978, 1.0565137843837575, 3.6352404391060613, 2.53774750717975, 3.2504868899700825, 1.9874198440814987, 0.7707253657578359, 2.797307384827306, 2.5483773451139538, 3.9048278693570166, 0.5053711386258443], 'episode_lengths': [288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288, 288]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.5278970407641334, 'mean_inference_ms': 7.27252415690882, 'mean_action_processing_ms': 0.23396732448518787, 'mean_env_wait_ms': 23.34069282278188, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.00899831453959147, 'StateBufferConnector_ms': 0.00612338383992513, 'ViewRequirementAgentConnector_ms': 0.1399695873260498}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 107202.667, 'learn_time_ms': 44308.724, 'learn_throughput': 90.276, 'synch_weights_time_ms': 4.985}, 'counters': {'num_env_steps_sampled': 4000, 'num_env_steps_trained': 4000, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 4000}, 'done': False, 'trial_id': '9d853_00000', 'perf': {'cpu_util_percent': 39.27777777777778, 'ram_util_percent': 23.075000000000003, 'gpu_util_percent0': 0.0, 'vram_util_percent0': 0.00013020833333333333}, 'experiment_tag': '0'}, error=RayTaskError(ValueError)(ValueError('Given checkpoint does not seem to be valid! No file with the name `algorithm_state.pkl` (or `checkpoint-[0-9]+`) found.')), log_dir=PosixPath('/home/ubuntu/ray_results/experiment_2023-04-04_02-58-45/experiment_9d853_00000_0_2023-04-04_02-58-45'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.get_best_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
