{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/sustaingym\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sustaingym.envs.evcharging import RealTraceGenerator\n",
    "from sustaingym.envs.evcharging.ev_charging import EVChargingEnv\n",
    "\n",
    "test_ranges = (\n",
    "    ('2019-05-01', '2019-08-31'),\n",
    "    ('2019-09-01', '2019-12-31'),\n",
    "    ('2020-02-01', '2020-05-31'),\n",
    "    ('2021-05-01', '2021-08-31'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 1.0, (54,), float32)\n",
      "(147,)\n",
      "created trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   3%|3         | 4608/150000 [09:36<5:03:04,  8.00it/s, env_step=4608, len=288, n/ep=16, n/st=4608, rew=6.98]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "{'gradient_step': 0, 'env_step': 4608, 'rew': 6.980065386343476, 'len': 288, 'n/ep': 16, 'n/st': 4608}\n",
      "{'duration': '1074.30s', 'train_time/model': '0.21s', 'test_step': 57600, 'test_episode': 200, 'test_time': '993.06s', 'test_speed': '58.00 step/s', 'best_reward': 5.925926711005014, 'best_result': '5.93 ± 3.60', 'train_step': 4608, 'train_episode': 16, 'train_time/collector': '81.03s', 'train_speed': '56.72 step/s'}\n",
      "{'best_result': '5.93 ± 3.60',\n",
      " 'best_reward': 5.925926711005014,\n",
      " 'duration': '1074.30s',\n",
      " 'test_episode': 200,\n",
      " 'test_speed': '58.00 step/s',\n",
      " 'test_step': 57600,\n",
      " 'test_time': '993.06s',\n",
      " 'train_episode': 16,\n",
      " 'train_speed': '56.72 step/s',\n",
      " 'train_step': 4608,\n",
      " 'train_time/collector': '81.03s',\n",
      " 'train_time/model': '0.21s'}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x3 and 147x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 215\u001b[0m\n\u001b[1;32m    211\u001b[0m     test_ppo(args)\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m     test_ppo()\n",
      "Cell \u001b[0;32mIn[3], line 204\u001b[0m, in \u001b[0;36mtest_ppo\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    202\u001b[0m policy\u001b[39m.\u001b[39meval()\n\u001b[1;32m    203\u001b[0m collector \u001b[39m=\u001b[39m Collector(policy, env)\n\u001b[0;32m--> 204\u001b[0m result \u001b[39m=\u001b[39m collector\u001b[39m.\u001b[39;49mcollect(n_episode\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, render\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mrender)\n\u001b[1;32m    205\u001b[0m rews, lens \u001b[39m=\u001b[39m result[\u001b[39m\"\u001b[39m\u001b[39mrews\u001b[39m\u001b[39m\"\u001b[39m], result[\u001b[39m\"\u001b[39m\u001b[39mlens\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    206\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinal reward: \u001b[39m\u001b[39m{\u001b[39;00mrews\u001b[39m.\u001b[39mmean()\u001b[39m}\u001b[39;00m\u001b[39m, length: \u001b[39m\u001b[39m{\u001b[39;00mlens\u001b[39m.\u001b[39mmean()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/data/collector.py:278\u001b[0m, in \u001b[0;36mCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m no_grad:\n\u001b[1;32m    276\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# faster than retain_grad version\u001b[39;00m\n\u001b[1;32m    277\u001b[0m         \u001b[39m# self.data.obs will be used by agent to get result\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, last_state)\n\u001b[1;32m    279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    280\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, last_state)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/policy/modelfree/pg.py:108\u001b[0m, in \u001b[0;36mPGPolicy.forward\u001b[0;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     batch: Batch,\n\u001b[1;32m     91\u001b[0m     state: Optional[Union[\u001b[39mdict\u001b[39m, Batch, np\u001b[39m.\u001b[39mndarray]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     93\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Batch:\n\u001b[1;32m     94\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute action over the given batch data.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[39m    :return: A :class:`~tianshou.data.Batch` which has 4 keys:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39m        more detailed explanation.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     logits, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor(batch\u001b[39m.\u001b[39;49mobs, state\u001b[39m=\u001b[39;49mstate, info\u001b[39m=\u001b[39;49mbatch\u001b[39m.\u001b[39;49minfo)\n\u001b[1;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(logits, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    110\u001b[0m         dist \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdist_fn(\u001b[39m*\u001b[39mlogits)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/utils/net/continuous.py:211\u001b[0m, in \u001b[0;36mActorProb.forward\u001b[0;34m(self, obs, state, info)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    205\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    206\u001b[0m     obs: Union[np\u001b[39m.\u001b[39mndarray, torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m    207\u001b[0m     state: Any \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    208\u001b[0m     info: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {},\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], Any]:\n\u001b[1;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mapping: obs -> logits -> (mu, sigma).\"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     logits, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(obs, state)\n\u001b[1;32m    212\u001b[0m     mu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmu(logits)\n\u001b[1;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbounded:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/utils/net/common.py:248\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, obs, state, info)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    243\u001b[0m     obs: Union[np\u001b[39m.\u001b[39mndarray, torch\u001b[39m.\u001b[39mTensor],\n\u001b[1;32m    244\u001b[0m     state: Any \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    245\u001b[0m     info: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {},\n\u001b[1;32m    246\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, Any]:\n\u001b[1;32m    247\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mapping: obs -> flatten (inside MLP)-> logits.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(obs)\n\u001b[1;32m    249\u001b[0m     bsz \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_dueling:  \u001b[39m# Dueling DQN\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/utils/net/common.py:145\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten_input:\n\u001b[1;32m    144\u001b[0m     obs \u001b[39m=\u001b[39m obs\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(obs)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x3 and 147x64)"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Independent, Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import ActorCritic, Net\n",
    "from tianshou.utils.net.continuous import ActorProb, Critic\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--task', type=str, default='Pendulum-v1')\n",
    "    parser.add_argument('--reward-threshold', type=float, default=None)\n",
    "    parser.add_argument('--seed', type=int, default=1)\n",
    "    parser.add_argument('--buffer-size', type=int, default=20000)\n",
    "    parser.add_argument('--lr', type=float, default=1e-3)\n",
    "    parser.add_argument('--gamma', type=float, default=0.95)\n",
    "    parser.add_argument('--epoch', type=int, default=5)\n",
    "    parser.add_argument('--step-per-epoch', type=int, default=150000)\n",
    "    parser.add_argument('--episode-per-collect', type=int, default=16)\n",
    "    parser.add_argument('--repeat-per-collect', type=int, default=2)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[64, 64])\n",
    "    parser.add_argument('--training-num', type=int, default=16)\n",
    "    parser.add_argument('--test-num', type=int, default=100)\n",
    "    parser.add_argument('--logdir', type=str, default='log')\n",
    "    parser.add_argument('--render', type=float, default=0.)\n",
    "    parser.add_argument(\n",
    "        '--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    # ppo special\n",
    "    parser.add_argument('--vf-coef', type=float, default=0.25)\n",
    "    parser.add_argument('--ent-coef', type=float, default=0.0)\n",
    "    parser.add_argument('--eps-clip', type=float, default=0.2)\n",
    "    parser.add_argument('--max-grad-norm', type=float, default=0.5)\n",
    "    parser.add_argument('--gae-lambda', type=float, default=0.95)\n",
    "    parser.add_argument('--rew-norm', type=int, default=1)\n",
    "    parser.add_argument('--dual-clip', type=float, default=None)\n",
    "    parser.add_argument('--value-clip', type=int, default=1)\n",
    "    parser.add_argument('--norm-adv', type=int, default=1)\n",
    "    parser.add_argument('--recompute-adv', type=int, default=0)\n",
    "    parser.add_argument('--resume', action=\"store_true\")\n",
    "    parser.add_argument(\"--save-interval\", type=int, default=4)\n",
    "    args = parser.parse_known_args()[0]\n",
    "    return args\n",
    "\n",
    "\n",
    "def test_ppo(args=get_args()):\n",
    "    env = EVChargingEnv(RealTraceGenerator('caltech', test_ranges[0]))\n",
    "    print(env.action_space)\n",
    "    print(env._vectorized_shape)\n",
    "    # return\n",
    "    # env = gym.make(args.task)\n",
    "    # args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "    args.state_shape = env._vectorized_shape # env.observation_space.shape or env.observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    args.max_action = env.action_space.high[0]\n",
    "    if args.reward_threshold is None:\n",
    "        default_reward_threshold = {\"Pendulum-v0\": -250, \"Pendulum-v1\": -250}\n",
    "        args.reward_threshold = default_reward_threshold.get(\n",
    "            args.task, 1_000 #env.spec.reward_threshold\n",
    "        )\n",
    "    # you can also use tianshou.env.SubprocVectorEnv\n",
    "    # train_envs = gym.make(args.task)\n",
    "    # train_envs = DummyVectorEnv(\n",
    "    #     [lambda: gym.make(args.task) for _ in range(args.training_num)]\n",
    "    # )\n",
    "    # # test_envs = gym.make(args.task)\n",
    "    # test_envs = DummyVectorEnv(\n",
    "    #     [lambda: gym.make(args.task) for _ in range(args.test_num)]\n",
    "    # )\n",
    "\n",
    "    train_envs = DummyVectorEnv([lambda: EVChargingEnv(RealTraceGenerator('caltech', test_ranges[0])) for _ in range(1)])\n",
    "    test_envs = DummyVectorEnv([lambda: EVChargingEnv(RealTraceGenerator('caltech', test_ranges[0])) for _ in range(1)])\n",
    "\n",
    "    # seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    train_envs.seed(args.seed)\n",
    "    test_envs.seed(args.seed)\n",
    "    # model\n",
    "    net = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=args.device)\n",
    "    actor = ActorProb(\n",
    "        net, args.action_shape, max_action=args.max_action, device=args.device\n",
    "    ).to(args.device)\n",
    "    critic = Critic(\n",
    "        Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=args.device),\n",
    "        device=args.device\n",
    "    ).to(args.device)\n",
    "    actor_critic = ActorCritic(actor, critic)\n",
    "    # orthogonal initialization\n",
    "    for m in actor_critic.modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            torch.nn.init.orthogonal_(m.weight)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    optim = torch.optim.Adam(actor_critic.parameters(), lr=args.lr)\n",
    "\n",
    "    # replace DiagGuassian with Independent(Normal) which is equivalent\n",
    "    # pass *logits to be consistent with policy.forward\n",
    "    def dist(*logits):\n",
    "        return Independent(Normal(*logits), 1)\n",
    "\n",
    "    policy = PPOPolicy(\n",
    "        actor,\n",
    "        critic,\n",
    "        optim,\n",
    "        dist,\n",
    "        discount_factor=args.gamma,\n",
    "        max_grad_norm=args.max_grad_norm,\n",
    "        eps_clip=args.eps_clip,\n",
    "        vf_coef=args.vf_coef,\n",
    "        ent_coef=args.ent_coef,\n",
    "        reward_normalization=args.rew_norm,\n",
    "        advantage_normalization=args.norm_adv,\n",
    "        recompute_advantage=args.recompute_adv,\n",
    "        dual_clip=args.dual_clip,\n",
    "        value_clip=args.value_clip,\n",
    "        gae_lambda=args.gae_lambda,\n",
    "        action_space=env.action_space,\n",
    "    )\n",
    "    # collector\n",
    "    train_collector = Collector(\n",
    "        policy, train_envs, VectorReplayBuffer(args.buffer_size, len(train_envs))\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs)\n",
    "    # log\n",
    "    log_path = os.path.join(args.logdir, args.task, \"ppo\")\n",
    "    writer = SummaryWriter(log_path)\n",
    "    logger = TensorboardLogger(writer, save_interval=args.save_interval)\n",
    "\n",
    "    def save_best_fn(policy):\n",
    "        torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= args.reward_threshold\n",
    "\n",
    "    def save_checkpoint_fn(epoch, env_step, gradient_step):\n",
    "        # see also: https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "        ckpt_path = os.path.join(log_path, \"checkpoint.pth\")\n",
    "        # Example: saving by epoch num\n",
    "        # ckpt_path = os.path.join(log_path, f\"checkpoint_{epoch}.pth\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model\": policy.state_dict(),\n",
    "                \"optim\": optim.state_dict(),\n",
    "            }, ckpt_path\n",
    "        )\n",
    "        return ckpt_path\n",
    "\n",
    "    if args.resume:\n",
    "        # load from existing checkpoint\n",
    "        print(f\"Loading agent under {log_path}\")\n",
    "        ckpt_path = os.path.join(log_path, \"checkpoint.pth\")\n",
    "        if os.path.exists(ckpt_path):\n",
    "            checkpoint = torch.load(ckpt_path, map_location=args.device)\n",
    "            policy.load_state_dict(checkpoint[\"model\"])\n",
    "            optim.load_state_dict(checkpoint[\"optim\"])\n",
    "            print(\"Successfully restore policy and optim.\")\n",
    "        else:\n",
    "            print(\"Fail to restore policy and optim.\")\n",
    "\n",
    "    # trainer\n",
    "    trainer = OnpolicyTrainer(\n",
    "        policy,\n",
    "        train_collector,\n",
    "        test_collector,\n",
    "        args.epoch,\n",
    "        args.step_per_epoch,\n",
    "        args.repeat_per_collect,\n",
    "        args.test_num,\n",
    "        args.batch_size,\n",
    "        episode_per_collect=args.episode_per_collect,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        logger=logger,\n",
    "        resume_from_log=args.resume,\n",
    "        save_checkpoint_fn=save_checkpoint_fn,\n",
    "    )\n",
    "    print(\"created trainer\")\n",
    "\n",
    "    for epoch, epoch_stat, info in trainer:\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        print(epoch_stat)\n",
    "        print(info)\n",
    "\n",
    "    assert stop_fn(info[\"best_reward\"])\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        pprint.pprint(info)\n",
    "        # Let's watch its performance!\n",
    "        env = gym.make(args.task)\n",
    "        policy.eval()\n",
    "        collector = Collector(policy, env)\n",
    "        result = collector.collect(n_episode=1, render=args.render)\n",
    "        rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "        print(f\"Final reward: {rews.mean()}, length: {lens.mean()}\")\n",
    "\n",
    "\n",
    "def test_ppo_resume(args=get_args()):\n",
    "    args.resume = True\n",
    "    test_ppo(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sustaingymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
