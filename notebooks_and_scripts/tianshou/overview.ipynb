{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/sustaingym\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "# import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import onpolicy_trainer\n",
    "from tianshou.utils.net.common import ActorCritic, Net\n",
    "from tianshou.utils.net.continuous import Actor, Critic\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# environments\n",
    "from sustaingym.envs.evcharging import RealTraceGenerator\n",
    "from sustaingym.envs.evcharging.ev_charging import EVChargingEnv\n",
    "\n",
    "test_ranges = (\n",
    "    ('2019-05-01', '2019-08-31'),\n",
    "    ('2019-09-01', '2019-12-31'),\n",
    "    ('2020-02-01', '2020-05-31'),\n",
    "    ('2021-05-01', '2021-08-31'),\n",
    ")\n",
    "\n",
    "env = EVChargingEnv(RealTraceGenerator('caltech', test_ranges[0]))\n",
    "train_envs = DummyVectorEnv([lambda: EVChargingEnv(RealTraceGenerator('caltech', test_ranges[0])) for _ in range(10)])\n",
    "test_envs = DummyVectorEnv([lambda: EVChargingEnv(RealTraceGenerator('caltech', test_ranges[0])) for _ in range(5)])\n",
    "# env = gym.make('CartPole-v0')\n",
    "# train_envs = DummyVectorEnv([lambda: gym.make('CartPole-v0') for _ in range(20)])\n",
    "# test_envs = DummyVectorEnv([lambda: gym.make('CartPole-v0') for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m test_collector \u001b[39m=\u001b[39m Collector(policy, test_envs)\n\u001b[1;32m     26\u001b[0m \u001b[39m# trainer\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m result \u001b[39m=\u001b[39m onpolicy_trainer(\n\u001b[1;32m     28\u001b[0m     policy,\n\u001b[1;32m     29\u001b[0m     train_collector,\n\u001b[1;32m     30\u001b[0m     test_collector,\n\u001b[1;32m     31\u001b[0m     max_epoch\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     32\u001b[0m     step_per_epoch\u001b[39m=\u001b[39;49m\u001b[39m50000\u001b[39;49m,\n\u001b[1;32m     33\u001b[0m     repeat_per_collect\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     34\u001b[0m     episode_per_test\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     35\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m,\n\u001b[1;32m     36\u001b[0m     step_per_collect\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m,\n\u001b[1;32m     37\u001b[0m     stop_fn\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m mean_reward: mean_reward \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m195\u001b[39;49m,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/trainer/onpolicy.py:150\u001b[0m, in \u001b[0;36monpolicy_trainer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39monpolicy_trainer\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Union[\u001b[39mfloat\u001b[39m, \u001b[39mstr\u001b[39m]]:  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Wrapper for OnpolicyTrainer run method.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[39m    It is identical to ``OnpolicyTrainer(...).run()``.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[1;32m    148\u001b[0m \u001b[39m    :return: See :func:`~tianshou.trainer.gather_info`.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[39mreturn\u001b[39;00m OnpolicyTrainer(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/trainer/base.py:441\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m     deque(\u001b[39mself\u001b[39;49m, maxlen\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)  \u001b[39m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     info \u001b[39m=\u001b[39m gather_info(\n\u001b[1;32m    443\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_time, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_collector, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector,\n\u001b[1;32m    444\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std\n\u001b[1;32m    445\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/trainer/base.py:252\u001b[0m, in \u001b[0;36mBaseTrainer.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m    253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/trainer/base.py:237\u001b[0m, in \u001b[0;36mBaseTrainer.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector, AsyncCollector)  \u001b[39m# Issue 700\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_collector\u001b[39m.\u001b[39mreset_stat()\n\u001b[0;32m--> 237\u001b[0m test_result \u001b[39m=\u001b[39m test_episode(\n\u001b[1;32m    238\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_collector, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_fn, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_epoch,\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepisode_per_test, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogger, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_metric\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    241\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_epoch\n\u001b[1;32m    242\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_reward_std \u001b[39m=\u001b[39m \\\n\u001b[1;32m    243\u001b[0m     test_result[\u001b[39m\"\u001b[39m\u001b[39mrew\u001b[39m\u001b[39m\"\u001b[39m], test_result[\u001b[39m\"\u001b[39m\u001b[39mrew_std\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/trainer/utils.py:27\u001b[0m, in \u001b[0;36mtest_episode\u001b[0;34m(policy, collector, test_fn, epoch, n_episode, logger, global_step, reward_metric)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m test_fn:\n\u001b[1;32m     26\u001b[0m     test_fn(epoch, global_step)\n\u001b[0;32m---> 27\u001b[0m result \u001b[39m=\u001b[39m collector\u001b[39m.\u001b[39;49mcollect(n_episode\u001b[39m=\u001b[39;49mn_episode)\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m reward_metric:\n\u001b[1;32m     29\u001b[0m     rew \u001b[39m=\u001b[39m reward_metric(result[\u001b[39m\"\u001b[39m\u001b[39mrews\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/data/collector.py:295\u001b[0m, in \u001b[0;36mCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m action_remap \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mmap_action(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mact)\n\u001b[1;32m    294\u001b[0m \u001b[39m# step in env\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m obs_next, rew, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m    296\u001b[0m     action_remap,  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    297\u001b[0m     ready_env_ids\n\u001b[1;32m    298\u001b[0m )\n\u001b[1;32m    299\u001b[0m done \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlogical_or(terminated, truncated)\n\u001b[1;32m    301\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    302\u001b[0m     obs_next\u001b[39m=\u001b[39mobs_next,\n\u001b[1;32m    303\u001b[0m     rew\u001b[39m=\u001b[39mrew,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     info\u001b[39m=\u001b[39minfo\n\u001b[1;32m    308\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/env/venvs.py:332\u001b[0m, in \u001b[0;36mBaseVectorEnv.step\u001b[0;34m(self, action, id)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_id(\u001b[39mid\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_async:\n\u001b[0;32m--> 332\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(action) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mid\u001b[39m)\n\u001b[1;32m    333\u001b[0m     \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mid\u001b[39m):\n\u001b[1;32m    334\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers[j]\u001b[39m.\u001b[39msend(action[i])\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model & optimizer\n",
    "\n",
    "# net = Net(env., hidden_sizes=[64, 64], device=device)\n",
    "# print(env.action_space.shape)\n",
    "# actor = Actor(net, 1, device=device).to(device)\n",
    "\n",
    "# model & optimizer\n",
    "net = Net(env._vectorized_shape, hidden_sizes=[64, 64], device=device)\n",
    "actor = Actor(net, env.action_space.shape, device=device).to(device)\n",
    "critic = Critic(net, device=device).to(device)\n",
    "actor_critic = ActorCritic(actor, critic)\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=0.0003)\n",
    "\n",
    "# PPO policy\n",
    "def dist_fn(logits):\n",
    "    return torch.distributions.Independent(torch.distributions.Normal(logits, 1), 1)\n",
    "\n",
    "dist = dist_fn #torch.distributions.Categorical() # torch.distributions.Categorical\n",
    "policy = PPOPolicy(actor, critic, optim, dist, action_space=env.action_space, deterministic_eval=True)\n",
    "        \n",
    "          \n",
    "# collector\n",
    "train_collector = Collector(policy, train_envs, VectorReplayBuffer(20000, len(train_envs)))\n",
    "test_collector = Collector(policy, test_envs)\n",
    "\n",
    "# trainer\n",
    "result = onpolicy_trainer(\n",
    "    policy,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    max_epoch=10,\n",
    "    step_per_epoch=50000,\n",
    "    repeat_per_collect=10,\n",
    "    episode_per_test=10,\n",
    "    batch_size=256,\n",
    "    step_per_collect=2000,\n",
    "    stop_fn=lambda mean_reward: mean_reward >= 195,\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, 1.0, (54,), float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(env._vectorized_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ubuntu/.conda/envs/sustaingymnasium/lib/python3.9/site-packages/tianshou/env/venvs.py\u001b[0m(332)\u001b[0;36mstep\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    330 \u001b[0;31m        \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    331 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 332 \u001b[0;31m            \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    333 \u001b[0;31m            \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    334 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "54\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "--KeyboardInterrupt--\n",
      "\n",
      "KeyboardInterrupt: Interrupted by user\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reward: 200.0, length: 200.0\n"
     ]
    }
   ],
   "source": [
    "# Let's watch its performance!\n",
    "policy.eval()\n",
    "result = test_collector.collect(n_episode=1, render=False)\n",
    "print(\"Final reward: {}, length: {}\".format(result[\"rews\"].mean(), result[\"lens\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.11\n"
     ]
    }
   ],
   "source": [
    "import tianshou\n",
    "print(tianshou.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sustaingymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ab2ea4418bedbee9b365d738d8cc70266502c8c268c2757070eda279d9f6b4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
